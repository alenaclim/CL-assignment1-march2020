{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group 4\n",
    "# Alena Clim: 2013151\n",
    "# Maartje van Berkel: 2010960\n",
    "# Merijn Broos: 2010284\n",
    "\n",
    "# We used a simple interpolation method for a bigram model.\n",
    "# I wasn't able to make it work generally, but it works for the specific value of n=2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think these blocks of code shouldn't have an influence on the way you'll test our code,\n",
    "# because you'll already have two separate files.\n",
    "# But I decided to let it here, so you can see the way we split the data\n",
    "\n",
    "# Opening a json file. The training set you gave us, I called it corpus and then split it\n",
    "with open('/_MYstuff/Desktop/Uni/Computational Linguistics/corpus.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# I am checking the length    \n",
    "length = len(data)\n",
    "print(\"Length corpus \", length)\n",
    "\n",
    "# Splitting a list of lists with the 80/20 percentage for train/test sets \n",
    "train = data[:int(length*0.8)]\n",
    "test = data[int(length*0.8):]\n",
    "\n",
    "# Here I waanted to make sure that I'm not missing elements, added they should be the previously printed length\n",
    "print(\"Length train \", len(train))\n",
    "print(\"Length test \", len(test))\n",
    "\n",
    "# This was the method I found to create two new json files.\n",
    "# The only problem is that if this piece of code is run multiple times it gives an error \n",
    "# because it's trying to create the files over and over again\n",
    "with open('training.json', 'x') as outfile:\n",
    "    json.dump(train, outfile) \n",
    "\n",
    "with open('test.json', 'x') as outfile:\n",
    "    json.dump(test, outfile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These were my paths after forming two new files from the original corpus\n",
    "\n",
    "train_path = '/_MYstuff/Desktop/Uni/Computational Linguistics/training.json'\n",
    "test_path = '/_MYstuff/Desktop/Uni/Computational Linguistics/test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I haven't changed anything here\n",
    "\n",
    "class Corpus(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    This class creates a corpus object read off a .json file consisting of a list of lists,\n",
    "    where each inner list is a sentence encoded as a list of strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path, t, n, bos_eos=True, vocab=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        DON'T TOUCH THIS CLASS! \n",
    "        IT'S HERE TO SHOW THE PROCESS, YOU DON'T NEED TO ANYTHING HERE. \n",
    "        \n",
    "        A Corpus object has the following attributes:\n",
    "         - vocab: set or None (default). If a set is passed, words in the input file not \n",
    "                         found in the set are replaced with the UNK string\n",
    "         - path: str, the path to the .json file used to build the corpus object\n",
    "         - t: int, words with frequency count < t are replaced with the UNK string\n",
    "         - ngram_size: int, 2 for bigrams, 3 for trigrams, and so on.\n",
    "         - bos_eos: bool, default to True. If False, bos and eos symbols are not \n",
    "                     prepended and appended to sentences.\n",
    "         - sentences: list of lists, containing the input sentences after lowercasing and \n",
    "                         splitting at the white space\n",
    "         - frequencies: Counter, mapping tokens to their frequency count in the corpus\n",
    "        \"\"\"\n",
    "        \n",
    "        self.vocab = vocab        \n",
    "        self.path = path\n",
    "        self.t = t\n",
    "        self.ngram_size = n\n",
    "        self.bos_eos = bos_eos\n",
    "        \n",
    "        self.sentences = self.read()\n",
    "        # output --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'park', '.'], ...]\n",
    "    \n",
    "        self.frequencies = self.freq_distr()\n",
    "        # output --> Counter('the': 485099, 'of': 301877, 'i': 286549, ...)\n",
    "        # the numbers are made up, they aren't the actual frequency counts\n",
    "        \n",
    "        if self.t or self.vocab:\n",
    "            # input --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'park', '.'], ...]\n",
    "            self.sentences = self.filter_words()\n",
    "            # output --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'UNK', '.'], ...]\n",
    "            # supposing that park wasn't frequent enough or was outside of the training \n",
    "            # vocabulary, it gets replaced by the UNK string\n",
    "            \n",
    "        if self.bos_eos:\n",
    "            # input --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'park', '.'], ...]\n",
    "            self.sentences = self.add_bos_eos()\n",
    "            # output --> [['bos', i', 'am', 'home' '.', 'eos'], \n",
    "            #             ['bos', you', 'went', 'to', 'the', 'park', '.', 'eos'], ...]\n",
    "                    \n",
    "    def read(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Reads the sentences off the .json file, replaces quotes, lowercases strings and splits \n",
    "        at the white space. Returns a list of lists.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.path.endswith('.json'):\n",
    "            sentences = json.load(open(self.path, 'r'))                \n",
    "        else:   \n",
    "            sentences = []\n",
    "            with open(self.path, 'r', encoding='latin-1') as f:\n",
    "                for line in f:\n",
    "                    print(line[:20])\n",
    "                    # first strip away newline symbols and the like, then replace ' and \" with the empty \n",
    "                    # string and get rid of possible remaining trailing spaces \n",
    "                    line = line.strip().translate({ord(i): None for i in '\"\\'\\\\'}).strip(' ')\n",
    "                    # lowercase and split at the white space (the corpus has ben previously tokenized)\n",
    "                    sentences.append(line.lower().split(' '))\n",
    "        \n",
    "        return sentences\n",
    "    \n",
    "    def freq_distr(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates a counter mapping tokens to frequency counts\n",
    "        \n",
    "        count = Counter()\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence:\n",
    "                count[w] += 1\n",
    "            \n",
    "        \"\"\"\n",
    "    \n",
    "        return Counter([word for sentence in self.sentences for word in sentence])\n",
    "        \n",
    "    \n",
    "    def filter_words(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Replaces illegal tokens with the UNK string. A token is illegal if its frequency count\n",
    "        is lower than the given threshold and/or if it falls outside the specified vocabulary.\n",
    "        The two filters can be both active at the same time but don't have to be. To exclude the \n",
    "        frequency filter, set t=0 in the class call.\n",
    "        \"\"\"\n",
    "        \n",
    "        filtered_sentences = []\n",
    "        for sentence in self.sentences:\n",
    "            filtered_sentence = []\n",
    "            for word in sentence:\n",
    "                if self.t and self.vocab:\n",
    "                    # check that the word is frequent enough and occurs in the vocabulary\n",
    "                    filtered_sentence.append(\n",
    "                        word if self.frequencies[word] > self.t and word in self.vocab else 'UNK'\n",
    "                    )\n",
    "                else:\n",
    "                    if self.t:\n",
    "                        # check that the word is frequent enough\n",
    "                        filtered_sentence.append(word if self.frequencies[word] > self.t else 'UNK')\n",
    "                    else:\n",
    "                        # check if the word occurs in the vocabulary\n",
    "                        filtered_sentence.append(word if word in self.vocab else 'UNK')\n",
    "                        \n",
    "            if len(filtered_sentence) > 1:\n",
    "                # make sure that the sentence contains more than 1 token\n",
    "                filtered_sentences.append(filtered_sentence)\n",
    "    \n",
    "        return filtered_sentences\n",
    "    \n",
    "    def add_bos_eos(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Adds the necessary number of BOS symbols and one EOS symbol.\n",
    "        \n",
    "        In a bigram model, you need one bos and one eos; in a trigram model you need two bos and one eos, \n",
    "        and so on...\n",
    "        \"\"\"\n",
    "        \n",
    "        padded_sentences = []\n",
    "        for sentence in self.sentences:\n",
    "            padded_sentence = ['#bos#']*(self.ngram_size-1) + sentence + ['#eos#']\n",
    "            padded_sentences.append(padded_sentence)\n",
    "    \n",
    "        return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this class I changed the get_ngram_probabilities function to return an interpolated bigram model.\n",
    "# I hardcored the values for lambda in the function so I didn't need to change the definition parameters.\n",
    "# Right now the lam parameter is not used (since it returns the same perplexity when run with different).\n",
    "# I decided to let it be in the definition for now, though, since I didn't make it general and since the lambdas are not dynamic as of now\n",
    "\n",
    "class LM(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a language model object which can be trained and tested.\n",
    "    The language model has the following attributes:\n",
    "     - vocab: set of strings\n",
    "     - lam: float, indicating the constant to add to transition counts to smooth them (default to 1)\n",
    "     - ngram_size: int, the size of the ngrams\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n, vocab=None, smooth='Laplace', lam=1):\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.lam = lam\n",
    "        self.ngram_size = n\n",
    "        \n",
    "    def get_ngram(self, sentence, i):\n",
    "        \n",
    "        \"\"\"\n",
    "        CHANGE AT OWN RISK.\n",
    "        \n",
    "        Takes in a list of string and an index, and returns the history and current \n",
    "        token of the appropriate size: the current token is the one at the provided \n",
    "        index, while the history consists of the n-1 previous tokens. If the ngram \n",
    "        size is 1, only the current token is returned.\n",
    "        \n",
    "        Example:\n",
    "        input sentence: ['bos', 'i', 'am', 'home', 'eos']\n",
    "        target index: 2\n",
    "        ngram size: 3\n",
    "        \n",
    "        ngram = ['bos', 'i', 'am']  \n",
    "        #from index 2-(3-1) = 0 to index i (the +1 is just because of how Python slices lists) \n",
    "        \n",
    "        history = ('bos', 'i')\n",
    "        target = 'am'\n",
    "        return (('bos', 'i'), 'am')\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.ngram_size == 1:\n",
    "            return sentence[i]\n",
    "        else:\n",
    "            ngram = sentence[i-(self.ngram_size-1):i+1]\n",
    "            history = tuple(ngram[:-1])\n",
    "            target = ngram[-1]\n",
    "            return (history, target)\n",
    "                    \n",
    "    def update_counts(self, corpus):\n",
    "        \n",
    "        \"\"\"\n",
    "        CHANGE AT OWN RISK.\n",
    "        \n",
    "        Creates a transition matrix with counts in the form of a default dict mapping history\n",
    "        states to current states to the co-occurrence count (unless the ngram size is 1, in which\n",
    "        case the transition matrix is a simple counter mapping tokens to frequencies. \n",
    "        The ngram size of the corpus object has to be the same as the language model ngram size.\n",
    "        The input corpus (passed by providing the corpus object) is processed by extracting ngrams\n",
    "        of the chosen size and updating transition counts.\n",
    "        \n",
    "        This method creates three attributes for the language model object:\n",
    "         - counts: dict, described above\n",
    "         - vocab: set, containing all the tokens in the corpus\n",
    "         - vocab_size: int, indicating the number of tokens in the vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.ngram_size != corpus.ngram_size:\n",
    "            raise ValueError(\"The corpus was pre-processed considering an ngram size of {} while the \"\n",
    "                             \"language model was created with an ngram size of {}. \\n\"\n",
    "                             \"Please choose the same ngram size for pre-processing the corpus and fitting \"\n",
    "                             \"the model.\".format(corpus.ngram_size, self.ngram_size))\n",
    "        \n",
    "        self.counts = defaultdict(dict) if self.ngram_size > 1 else Counter()\n",
    "        for sentence in corpus.sentences:\n",
    "            for idx in range(self.ngram_size-1, len(sentence)):\n",
    "                ngram = self.get_ngram(sentence, idx)\n",
    "                if self.ngram_size == 1:\n",
    "                    self.counts[ngram] += 1\n",
    "                else:\n",
    "                    # it's faster to try to do something and catch an exception than to use an if statement to check\n",
    "                    # whether a condition is met beforehand. The if is checked everytime, the exception is only catched\n",
    "                    # the first time, after that everything runs smoothly\n",
    "                    try:\n",
    "                        self.counts[ngram[0]][ngram[1]] += 1\n",
    "                    except KeyError:\n",
    "                        self.counts[ngram[0]][ngram[1]] = 1\n",
    "        \n",
    "        # first loop through the sentences in the corpus, then loop through each word in a sentence\n",
    "        self.vocab = {word for sentence in corpus.sentences for word in sentence}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "    \n",
    "    def get_unigram_probability(self, ngram):\n",
    "        \n",
    "        \"\"\"\n",
    "        CHANGE THIS.\n",
    "        \n",
    "        Compute the probability of a given unigram in the estimated language model using\n",
    "        Laplace smoothing (add k).\n",
    "        \n",
    "        I didn't change this. I took parts of this code, but I copied it and let this function untouched.\n",
    "        \"\"\"\n",
    "        \n",
    "        tot = sum(list(self.counts.values())) + (self.vocab_size*self.lam)\n",
    "        try:\n",
    "            ngram_count = self.counts[ngram] + self.lam\n",
    "        except KeyError:\n",
    "            ngram_count = self.lam\n",
    "            print(ngram_count, tot)\n",
    "        \n",
    "        prob = ngram_count/tot\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def get_ngram_probability(self, history, target):\n",
    "        \n",
    "        \"\"\"\n",
    "        CHANGE THIS.\n",
    "        \n",
    "        Compute the conditional probability of the target token given the history, using \n",
    "        Laplace smoothing (add k).\n",
    "        \n",
    "        We used the interpolation method, implementing the formula found in this document that explained the proper method\n",
    "        (I'm not sure how to properly reference something in code, but this is the link, and the formula is at page 8):\n",
    "        (The document is called \"nlp-programming-en-02-bigramlm.pdf\", in case you want to google it, since the link returns a pdf)\n",
    "        \n",
    "        https://www.google.ro/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwiYsvbl967oAhWS6aQKHW3sD3cQFjAAegQIBRAB&url=http%3A%2F%2Fwww.phontron.com%2Fslides%2Fnlp-programming-en-02-bigramlm.pdf&usg=AOvVaw3MjuGgJ99lCXYeUwbmGpUI\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        lambda_1 = 0.85   # weight for the unigrams\n",
    "        lambda_2 = 0.85   # weight for the bigrams \n",
    "        \n",
    "        # This is for the bigrams \n",
    "        try:\n",
    "            ngram_tot = np.sum(list(self.counts[history].values()))\n",
    "            try:\n",
    "                # I deleted the +self.lam because I'm not using Laplace anymore, so I don't need to add lam\n",
    "                # I decided to keep the form of the try/except statements though, since I'm not familiar enought with them to change them myself\n",
    "                transition_count = self.counts[history][target]\n",
    "            except KeyError:\n",
    "                # I kept all these excepts and added print statements throughout my debugging process\n",
    "                # I also decided to have them here just in case some exceptions are raised, and the overall\n",
    "                # probability will not be zero anyways, because I'm adding together the unigrams and bigrams too\n",
    "                transition_count = 0\n",
    "        except KeyError:\n",
    "            transition_count = 0 \n",
    "            ngram_tot = 0 \n",
    "            print(\"Bigram exeption!!!\") # This was here just to check if it entered this branch\n",
    "\n",
    "        try:\n",
    "            # I used counts[target] so I'm also getting a dict with the values, instead of only an int\n",
    "            # It took me a long while to figure out a way to avoif an int+dict error (trying to add together the uni and bi gram probab)\n",
    "            unigram = sum(list(self.counts[target].values())) \n",
    "        except:\n",
    "            print(\"Unigram exception!!!\") # This was testing if this branch got called\n",
    "        \n",
    "        # This was implemented following the mathematical formula found in the previously referenced document\n",
    "        # Specifically, an interpolated bigram model would need weights for both the bigrams and the unigrams\n",
    "        # P(w_i|w_i-1) = lambda2 * P_ML(w_i|w_i-1) + (1-lambda2) * P(w_i) + lambda1 * P_ML(w_i) + (1-lambda1) * 1/N\n",
    "        return lambda_2 * (transition_count/ngram_tot) + (1-lambda_2) * ((lambda_1 * unigram/self.vocab_size)) + (1-lambda_1)/self.vocab_size #unigram_probability\n",
    "    \n",
    "\n",
    "    def perplexity(self, test_corpus):\n",
    "        \n",
    "        \"\"\"\n",
    "        Uses the estimated language model to process a corpus and computes the perplexity \n",
    "        of the language model over the corpus.\n",
    "        \n",
    "        DON'T TOUCH THIS FUNCTION!!!\n",
    "        \"\"\"\n",
    "        \n",
    "        probs = []\n",
    "        for sentence in test_corpus.sentences:\n",
    "            for idx in range(self.ngram_size-1, len(sentence)):\n",
    "                ngram = self.get_ngram(sentence, idx)\n",
    "                if self.ngram_size == 1:\n",
    "                    probs.append(self.get_unigram_probability(ngram))\n",
    "                else:\n",
    "                    probs.append(self.get_ngram_probability(ngram[0], ngram[1]))\n",
    "        \n",
    "        entropy = np.log2(probs)\n",
    "        # this assertion makes sure that you retrieved valid probabilities, whose log must be <= 0\n",
    "        assert all(entropy <= 0)\n",
    "        \n",
    "        avg_entropy = -1 * (sum(entropy) / len(entropy))\n",
    "        \n",
    "        return pow(2.0, avg_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, we tried to increase the size of n (1-6). We got the following perplexities:\n",
    "\n",
    "# n=1 => Perplex = 744.4517511377531\n",
    "# n=2 => Perplex = 325.55512508479444  (the baseline)\n",
    "\n",
    "# After this the scores become extremely large, because I don't think the corpus is large enough\n",
    "# I tried them just for check for possible saturation points of the curve\n",
    "\n",
    "# n=3 => Perplex = 1271.9313925486588\n",
    "# n=4 => Perplex = 5826.303193446068\n",
    "# n=5 => Perplex = 12004.074900534266\n",
    "# n=6 => Perplex = 14942.630558523799"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secondly, we tried to change the lam parameter on the bigram model (since that one had the best perplexity this far):\n",
    "\n",
    "# lam=0.001 => Perplex = 325.55512508479444  (the baseline)\n",
    "# lam=0.01 => Perplex = 309.76113373959845  (this score is even better than the baseline! can we beat this?)\n",
    "# lam=0.1 => Perplex = 415.2798099772183\n",
    "# lam=1 => Perplex = 813.845075367734"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code snippets for the above steps can be found at the end of the file. \n",
    "# After these we decided to try the interpolation method on the bigram model.\n",
    "# Below is the code with all the parameters already assigned (lambdas are hardcored in the actual function for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram model with interpolation. Frequency threshold of 10 is applied.\n",
    "\n",
    "n = 2\n",
    "train_corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "bigram_model = LM(n, lam=0.001) # lam doesn't matter here, since I'm not using Laplace anymore\n",
    "bigram_model.update_counts(train_corpus)\n",
    "\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=bigram_model.vocab)\n",
    "bigram_model.perplexity(test_corpus)\n",
    "\n",
    "# For this, the perplexity score is 297.44508617084915. So we also managed to beat the Laplace with add 0.01. YAY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From this point lower, the code we used to get the above scores can be noticed. \n",
    "# I wouldn't recomment running it, since it takes a huge amount of time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-a922434fbe8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# are replaced with the UNK string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbos_eos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0munigram_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0munigram_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-06c120e0f57a>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, t, n, bos_eos, vocab)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# output --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'park', '.'], ...]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfreq_distr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;31m# output --> Counter('the': 485099, 'of': 301877, 'i': 286549, ...)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# the numbers are made up, they aren't the actual frequency counts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-06c120e0f57a>\u001b[0m in \u001b[0;36mfreq_distr\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \"\"\"\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\_MYstuff\\Downloads\\WPy64-3760\\python-3.7.6.amd64\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\_MYstuff\\Downloads\\WPy64-3760\\python-3.7.6.amd64\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    654\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m                 \u001b[0m_count_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 656\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# example code to run a unigram model with add 0.001 smoothing. Tokens with a frequency count lower than 10\n",
    "# are replaced with the UNK string\n",
    "n = 1\n",
    "train_corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "unigram_model = LM(n, lam=0.001)\n",
    "unigram_model.update_counts(train_corpus)\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=unigram_model.vocab)\n",
    "unigram_model.perplexity(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297.44508617084915"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example code to run a bigram model with add 0.001 smoothing. The same frequency threshold is applied.\n",
    "n = 2\n",
    "train_corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "bigram_model = LM(n, lam=0.001)\n",
    "bigram_model.update_counts(train_corpus)\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=bigram_model.vocab)\n",
    "bigram_model.perplexity(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output perplexity is 325.55512508479444\n",
    "# This is the baseline score that I have to beat (so I have to have a perplexity that's lower than 325)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.99984504867686"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example code to run a trigram model with add 0.001 smoothing. The same frequency threshold is applied.\n",
    "n = 3\n",
    "train_corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "trigram_model = LM(n, lam=0.001)\n",
    "trigram_model.update_counts(train_corpus)\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=trigram_model.vocab)\n",
    "trigram_model.perplexity(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110.9697497960886"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example code to run a fourgram model with add 0.001 smoothing. The same frequency threshold is applied.\n",
    "n = 4\n",
    "train_corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "fourgram_model = LM(n, lam=0.001)\n",
    "fourgram_model.update_counts(train_corpus)\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=fourgram_model.vocab)\n",
    "fourgram_model.perplexity(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output perplexity is 5826.303193446068"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151.89199446403902"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example code to run a fivegram model with add 0.001 smoothing. The same frequency threshold is applied.\n",
    "n = 5\n",
    "train_corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "fivegram_model = LM(n, lam=0.001)\n",
    "fivegram_model.update_counts(train_corpus)\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=fivegram_model.vocab)\n",
    "fivegram_model.perplexity(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output perplexity is 12004.074900534266"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161.2159539649579"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example code to run a sixgram model with add 0.001 smoothing. The same frequency threshold is applied.\n",
    "n = 6\n",
    "train_corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "sixgram_model = LM(n, lam=0.001)\n",
    "sixgram_model.update_counts(train_corpus)\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=sixgram_model.vocab)\n",
    "sixgram_model.perplexity(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output perplexity is 14942.630558523799"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the lam=0.001 of the model to see if the perplexity lowers for a bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297.44508617084915"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example code to run a bigram model with add 0.01 smoothing. The same frequency threshold is applied.\n",
    "n = 2\n",
    "train_corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "bigram_model_01 = LM(n, lam=0.01)\n",
    "bigram_model_01.update_counts(train_corpus)\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=bigram_model_01.vocab)\n",
    "bigram_model_01.perplexity(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output perplexity is 309.76113373959845\n",
    "# This is the best perplexity this far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-f44b631e5b9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtest_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbos_eos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbigram_model_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mbigram_model_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-1c3ad0cdeb16>\u001b[0m in \u001b[0;36mperplexity\u001b[1;34m(self, test_corpus)\u001b[0m\n\u001b[0;32m    157\u001b[0m                     \u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_unigram_probability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m                     \u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_ngram_probability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[0mentropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-1c3ad0cdeb16>\u001b[0m in \u001b[0;36mget_ngram_probability\u001b[1;34m(self, history, target)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \"\"\"\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mtot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mngram_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'dict'"
     ]
    }
   ],
   "source": [
    "# example code to run a bigram model with add 0.1 smoothing. The same frequency threshold is applied.\n",
    "n = 2\n",
    "train_corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "bigram_model_1 = LM(n, lam=0.1)\n",
    "bigram_model_1.update_counts(train_corpus)\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=bigram_model_1.vocab)\n",
    "bigram_model_1.perplexity(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output perplexity is 415.2798099772183"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.219167010579074"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example code to run a bigram model with add 1.0 smoothing. The same frequency threshold is applied.\n",
    "n = 2\n",
    "train_corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "bigram_model_11 = LM(n, lam=1.0)\n",
    "bigram_model_11.update_counts(train_corpus)\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=bigram_model_11.vocab)\n",
    "bigram_model_11.perplexity(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output perplexity is 813.845075367734"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
